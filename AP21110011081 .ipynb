{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","authorship_tag":"ABX9TyNnCi3c9iMKS7IlmMD3XF5X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bpr4XXKoS9oH","executionInfo":{"status":"ok","timestamp":1732456446036,"user_tz":-330,"elapsed":1603504,"user":{"displayName":"Mohan Vignesh","userId":"17738547519427145923"}},"outputId":"fafd8433-6cec-4109-e8bb-3b60a56c4140"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30, Accuracy: 61.84%\n","Epoch 2/30, Accuracy: 71.10%\n","Epoch 3/30, Accuracy: 72.92%\n","Epoch 4/30, Accuracy: 74.90%\n","Epoch 5/30, Accuracy: 76.22%\n","Epoch 6/30, Accuracy: 76.98%\n","Epoch 7/30, Accuracy: 77.46%\n","Epoch 8/30, Accuracy: 77.92%\n","Epoch 9/30, Accuracy: 78.02%\n","Epoch 10/30, Accuracy: 78.70%\n","Epoch 11/30, Accuracy: 78.72%\n","Epoch 12/30, Accuracy: 78.76%\n","Epoch 13/30, Accuracy: 79.46%\n","Epoch 14/30, Accuracy: 79.48%\n","Epoch 15/30, Accuracy: 79.42%\n","Epoch 16/30, Accuracy: 80.02%\n","Epoch 17/30, Accuracy: 79.92%\n","Epoch 18/30, Accuracy: 80.28%\n","Epoch 19/30, Accuracy: 80.36%\n","Epoch 20/30, Accuracy: 80.90%\n","Epoch 21/30, Accuracy: 81.02%\n","Epoch 22/30, Accuracy: 80.98%\n","Epoch 23/30, Accuracy: 81.50%\n","Epoch 24/30, Accuracy: 81.46%\n","Epoch 25/30, Accuracy: 81.56%\n","Epoch 26/30, Accuracy: 82.00%\n","Epoch 27/30, Accuracy: 82.54%\n","Epoch 28/30, Accuracy: 82.34%\n","Epoch 29/30, Accuracy: 82.76%\n","Epoch 30/30, Accuracy: 82.78%\n","Final Test Accuracy: 81.70%\n"]}],"source":["import numpy as np\n","import random\n","\n","# Triangular Membership Function: Defines how fuzzy a pixel value is within certain ranges\n","def triangular_membership(x, a, b, c):\n","    if x < a or x > c:\n","        return 0\n","    elif x <= b:\n","        return (x - a) / (b - a) if b != a else 0\n","    else:\n","        return (c - x) / (c - b) if c != b else 0\n","\n","# Fuzzify an Image: Fuzzifies an image by assigning fuzzy values for light, medium, and dark intensities\n","def fuzzify_image(image):\n","    light = np.zeros_like(image, dtype=float)\n","    medium = np.zeros_like(image, dtype=float)\n","    dark = np.zeros_like(image, dtype=float)\n","\n","    # Loop over each pixel and fuzzify its value\n","    for i in range(image.shape[0]):\n","        for j in range(image.shape[1]):\n","            pixel_value = image[i, j]\n","            light[i, j] = triangular_membership(pixel_value, 0, 0, 85)\n","            medium[i, j] = triangular_membership(pixel_value, 85, 130, 175)\n","            dark[i, j] = triangular_membership(pixel_value, 175, 255, 255)\n","\n","    # Stack the fuzzy values (light, medium, dark) to create a 3-channel fuzzified image\n","    return np.stack([light, medium, dark], axis=-1)\n","\n","# Preprocess Dataset: Select a random subset of images and labels and fuzzify the images\n","def preprocess_dataset(x_data, y_data, num_samples=1000):\n","    selected_indices = random.sample(range(len(x_data)), num_samples)\n","    selected_images = x_data[selected_indices]\n","    selected_labels = y_data[selected_indices]\n","\n","    # Fuzzify the images\n","    fuzzified_images = np.array([fuzzify_image(img) for img in selected_images])\n","\n","    # One-hot encode the labels\n","    fuzzified_labels = np.zeros((len(selected_labels), 10))\n","    for i, label in enumerate(selected_labels):\n","        fuzzified_labels[i, label] = 1\n","\n","    # Normalize fuzzified images\n","    fuzzified_images /= np.max(fuzzified_images)\n","    return fuzzified_images, fuzzified_labels\n","\n","# Xavier Initialization: A method for initializing weights in neural networks\n","def xavier_init(shape):\n","    return np.random.randn(*shape) * np.sqrt(2 / sum(shape))\n","\n","# Convolution Operation: Applies a convolution operation between the image and a kernel\n","def convolve2d(image, kernel):\n","    kernel_height, kernel_width = kernel.shape\n","    img_height, img_width = image.shape\n","\n","    output_height = img_height - kernel_height + 1\n","    output_width = img_width - kernel_width + 1\n","\n","    output = np.zeros((output_height, output_width))\n","\n","    for i in range(output_height):\n","        for j in range(output_width):\n","            region = image[i:i + kernel_height, j:j + kernel_width]\n","            output[i, j] = np.sum(region * kernel)\n","\n","    return output\n","\n","# Max Pooling: Applies max pooling to reduce the spatial dimensions of the feature map\n","def max_pooling(image, pool_size=2):\n","    output_height = image.shape[0] // pool_size\n","    output_width = image.shape[1] // pool_size\n","    pooled = np.zeros((output_height, output_width))\n","\n","    for i in range(output_height):\n","        for j in range(output_width):\n","            region = image[i * pool_size:(i + 1) * pool_size, j * pool_size:(j + 1) * pool_size]\n","            pooled[i, j] = np.max(region)\n","    return pooled\n","\n","# ReLU Activation: Applies ReLU activation function\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","# Softmax Activation: Converts logits into probabilities\n","def softmax(x):\n","    exp_x = np.exp(x - np.max(x))  # Numerical stability\n","    return exp_x / np.sum(exp_x)\n","\n","# Cross-Entropy Loss: Computes the cross-entropy loss between predictions and actual labels\n","def cross_entropy(predictions, targets):\n","    return -np.sum(targets * np.log(predictions + 1e-9)) / len(targets)\n","\n","# Forward Pass: Performs the forward pass of the neural network\n","def forward_pass(image, params):\n","    # Convolutional Layers\n","    conv1 = relu(convolve2d(image, params[\"conv1_kernel\"]))\n","    pool1 = max_pooling(conv1)\n","\n","    conv2 = relu(convolve2d(pool1, params[\"conv2_kernel\"]))\n","    pool2 = max_pooling(conv2)\n","\n","    # Flatten Layer: Converts the pooled output into a 1D vector\n","    flattened = pool2.flatten()\n","\n","    # Fully Connected Layers\n","    fc1 = relu(np.dot(flattened, params[\"fc1_weights\"]) + params[\"fc1_biases\"])\n","    logits = np.dot(fc1, params[\"fc2_weights\"]) + params[\"fc2_biases\"]\n","\n","    # Output Layer with Softmax\n","    output = softmax(logits)\n","    return output, fc1, flattened\n","\n","# Backpropagation: Computes gradients for backpropagation\n","def backpropagation(output, label, fc1, flattened, params, gradients):\n","    d_logits = output - label  # Derivative of softmax with cross-entropy\n","\n","    # Gradients for the second fully connected layer\n","    gradients[\"fc2_weights\"] += np.outer(fc1, d_logits)\n","    gradients[\"fc2_biases\"] += d_logits\n","\n","    # Gradients for the first fully connected layer\n","    d_fc1 = np.dot(d_logits, params[\"fc2_weights\"].T) * (fc1 > 0)  # ReLU derivative\n","    gradients[\"fc1_weights\"] += np.outer(flattened, d_fc1)\n","    gradients[\"fc1_biases\"] += d_fc1\n","\n","    return gradients\n","\n","# Update Parameters: Updates the network parameters using the computed gradients\n","def update_parameters(params, gradients, learning_rate):\n","    for key in gradients.keys():\n","        params[key] -= learning_rate * gradients[key]\n","    return params\n","\n","# Load MNIST Dataset: Loads the MNIST dataset and normalizes the images\n","def load_mnist_data():\n","    from tensorflow.keras.datasets import mnist\n","    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","    x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize images\n","    return x_train, y_train, x_test, y_test\n","\n","# Initialize Parameters: Initializes network parameters\n","def initialize_parameters(input_size):\n","    params = {\n","        \"conv1_kernel\": xavier_init((3, 3)),\n","        \"conv2_kernel\": xavier_init((3, 3)),\n","        \"fc1_weights\": xavier_init((input_size, 128)),\n","        \"fc1_biases\": np.zeros(128),\n","        \"fc2_weights\": xavier_init((128, 10)),\n","        \"fc2_biases\": np.zeros(10),\n","    }\n","    return params\n","\n","# Accuracy Calculation: Computes accuracy by comparing predicted and true labels\n","def calculate_accuracy(images, labels, params):\n","    correct = 0\n","    total = len(images)\n","\n","    for img, label in zip(images, labels):\n","        output, _, _ = forward_pass(img[:, :, 0], params)\n","        predicted = np.argmax(output)\n","        true_label = np.argmax(label)\n","        if predicted == true_label:\n","            correct += 1\n","\n","    return correct / total\n","\n","# Training Loop: Trains the model using stochastic gradient descent\n","def train_model(images, labels, params, epochs=30, batch_size=32, learning_rate=0.0005):\n","    for epoch in range(epochs):\n","        indices = np.arange(len(images))\n","        np.random.shuffle(indices)\n","        images, labels = images[indices], labels[indices]\n","\n","        for i in range(0, len(images), batch_size):\n","            batch_images = images[i:i + batch_size]\n","            batch_labels = labels[i:i + batch_size]\n","\n","            gradients = {key: np.zeros_like(value) for key, value in params.items()}\n","\n","            for img, label in zip(batch_images, batch_labels):\n","                output, fc1, flattened = forward_pass(img[:, :, 0], params)\n","                gradients = backpropagation(output, label, fc1, flattened, params, gradients)\n","\n","            # Update Parameters after processing the batch\n","            params = update_parameters(params, gradients, learning_rate)\n","\n","        # Calculate and print accuracy after each epoch\n","        epoch_accuracy = calculate_accuracy(images, labels, params)\n","        print(f\"Epoch {epoch+1}/{epochs}, Accuracy: {epoch_accuracy * 100:.2f}%\")\n","\n","    return params\n","\n","# Main Block: Loads data, preprocesses it, initializes parameters, and trains the model\n","if __name__ == \"__main__\":\n","    # Load Dataset\n","    x_train, y_train, x_test, y_test = load_mnist_data()\n","\n","    # Preprocess Training Data (Fuzzified images)\n","    fuzzified_train, fuzzified_labels = preprocess_dataset(x_train, y_train, 5000)\n","\n","    # Determine Input Size for the Fully Connected Layer\n","    sample_img = fuzzified_train[0, :, :, 0]\n","    conv1 = convolve2d(sample_img, np.random.randn(3, 3))\n","    pool1 = max_pooling(conv1)\n","    conv2 = convolve2d(pool1, np.random.randn(3, 3))\n","    pool2 = max_pooling(conv2)\n","    input_size = pool2.flatten().size\n","\n","    # Initialize Parameters (weights and biases)\n","    params = initialize_parameters(input_size)\n","\n","    # Train the Model\n","    params = train_model(fuzzified_train, fuzzified_labels, params, epochs=30, batch_size=32)\n","\n","    # Evaluate the Model on Test Data\n","    fuzzified_test, test_labels = preprocess_dataset(x_test, y_test, 1000)\n","    predictions = []\n","    for img in fuzzified_test:\n","        output, _, _ = forward_pass(img[:, :, 0], params)\n","        predictions.append(np.argmax(output))\n","\n","    # Calculate final accuracy\n","    accuracy = np.mean(np.argmax(test_labels, axis=1) == predictions)\n","    print(f\"Final Test Accuracy: {accuracy * 100:.2f}%\")\n"]}]}